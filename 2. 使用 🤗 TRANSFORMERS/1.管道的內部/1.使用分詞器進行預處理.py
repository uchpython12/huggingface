from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)

# è¼¸å‡ºæœ¬èº«æ˜¯ä¸€å€‹åŒ…å«å…©å€‹éµçš„å­—å…¸ï¼Œinput_idså’Œattention_maskã€‚
# input_idsåŒ…å«å…©è¡Œæ•´æ•¸ï¼ˆæ¯å€‹å¥å­ä¸€è¡Œï¼‰ï¼Œå®ƒå€‘æ˜¯æ¯å€‹å¥å­ä¸­æ¨™è¨˜çš„å”¯ä¸€æ¨™è¨˜ï¼ˆtokenï¼‰ã€‚
# æˆ‘å€‘å°‡åœ¨æœ¬ç« å¾Œé¢è§£é‡‹ä»€éº¼æ˜¯attention_maskã€‚

# ç€è¦½æ¨¡å‹

from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)

# é«˜ç¶­å‘é‡ï¼Ÿ
# Transformers æ¨¡å¡Šçš„å‘é‡è¼¸å‡ºé€šå¸¸è¼ƒå¤§ã€‚å®ƒé€šå¸¸æœ‰ä¸‰å€‹ç¶­åº¦ï¼š
#
# Batch size: ä¸€æ¬¡è™•ç†çš„åºåˆ—æ•¸ï¼ˆåœ¨æˆ‘å€‘çš„ç¤ºä¾‹ä¸­ç‚º2ï¼‰ã€‚
# Sequence length: åºåˆ—çš„æ•¸å€¼è¡¨ç¤ºçš„é•·åº¦ï¼ˆåœ¨æˆ‘å€‘çš„ç¤ºä¾‹ä¸­ç‚º16ï¼‰ã€‚
# Hidden size: æ¯å€‹æ¨¡å‹è¼¸å…¥çš„å‘é‡ç¶­åº¦ã€‚

outputs = model(**inputs)
print(outputs.last_hidden_state.shape)

# ğŸ¤— Transformersä¸­æœ‰è¨±å¤šä¸åŒçš„é«”ç³»çµæ§‹ï¼Œæ¯ç¨®é«”ç³»çµæ§‹éƒ½æ˜¯åœç¹è™•ç†ç‰¹å®šä»»å‹™è€Œè¨­è¨ˆçš„ã€‚ä»¥ä¸‹æ˜¯ä¸€å€‹éè©³ç›¡çš„åˆ—è¡¨ï¼š
#
# *Model (retrieve the hidden states)
# *ForCausalLM
# *ForMaskedLM
# *ForMultipleChoice
# *ForQuestionAnswering
# *ForSequenceClassification
# *ForTokenClassification
# ä»¥åŠå…¶ä»– ğŸ¤—

from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)

print(outputs.logits.shape)

print(outputs.logits)

import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)

model.config.id2label